{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a582649d-069f-42c3-b8e8-dcf8e39db5ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Phase 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6ad32805-4a7e-4805-a42c-8471a9256885",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "%pip install requests pandas pyspark\n",
    "# in case modules are not intalled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584bb1bc-6641-471a-9289-360f1ecf091b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bronze Ingestion"
    }
   },
   "source": [
    "# Phase 2: Medallion Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9b1fdc2b-9b15-4466-8055-61cdaeef0230",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Bronze Layer (Ingestion)"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "from pyspark.sql.functions import col, from_unixtime, to_date\n",
    "\n",
    "# 1. Define the API and Coins\n",
    "# Coingecko allows historical fetch without api key\n",
    "coins = [\"bitcoin\", \"ethereum\", \"solana\"]\n",
    "base_url = \"https://api.coingecko.com/api/v3/coins/{}/market_chart?vs_currency=usd&days={}\"\n",
    "days = \"365\"\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 2. Loop through coins and fetch data\n",
    "print(\"Starting Ingestion\")\n",
    "for coin in coins:\n",
    "    url = base_url.format(coin, days)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "\n",
    "        # The API returns a list of [timestamp (ms), price], we need to parse that\n",
    "        if 'prices' in data:\n",
    "            for entry in data['prices']:\n",
    "                # entry[0] is timestamp, entry[1] is price\n",
    "                all_data.append({\n",
    "                    \"coin_id\": coin,\n",
    "                    \"timestamp_ms\": entry[0],\n",
    "                    \"price_usd\": entry[1],\n",
    "                    \"source\": \"coingecko_api\"\n",
    "                })\n",
    "            print(f\"Success {coin}\")\n",
    "        else:\n",
    "            print(f\"Error fetching {coin}: {data}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch {coin}: {e}\")\n",
    "\n",
    "# 3. Convert to Pandas first (easier for JSON lists), then Spark\n",
    "pdf = pd.DataFrame(all_data)\n",
    "\n",
    "## Define schema explicitly to avoid data type issues\n",
    "schema = StructType([\n",
    "    StructField(\"coin_id\", StringType(), True),\n",
    "    StructField(\"timestamp_ms\", LongType(), True),\n",
    "    StructField(\"price_usd\", DoubleType(), True),\n",
    "    StructField(\"source\", StringType(), True)\n",
    "])\n",
    "\n",
    "## Create Spark DataFrame\n",
    "raw_df = spark.createDataFrame(pdf, schema=schema)\n",
    "\n",
    "# 4. Add human-redeable dates (Transformation)\n",
    "bronze_df = raw_df \\\n",
    "    .withColumn(\"event_time\", (col(\"timestamp_ms\") / 1000).cast(TimestampType())) \\\n",
    "    .withColumn(\"date\", to_date(col(\"event_time\")))\n",
    "\n",
    "# 5. Write to Delta Lake (Bonze Layer)\n",
    "# We use 'overwrite' for this initial load. for daily updates, we would use 'append'\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze_crypto_prices\")\n",
    "\n",
    "print(f\"Success! {bronze_df.count()} records written to 'bronze_crypto_prices'.\")\n",
    "display(bronze_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2498f9dd-8841-48a2-942b-6e37b094315b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In Databricks, it is very common to use Python for the complicated API work (Bronze) and then switch to SQL for the transformations (Silver/Gold) because SQL is incredibly powerful for data manipulation and easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e21d049c-ce4f-487d-9761-74695ca2fbab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Silver Layer (cleaning-transformation)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- 1. Create the Silver Table structure (if it doesn't exist yet)\n",
    "CREATE TABLE IF NOT EXISTS silver_crypto_prices (\n",
    "  coin_id STRING,\n",
    "  event_time TIMESTAMP,\n",
    "  date DATE,\n",
    "  price_usd DOUBLE,\n",
    "  ingest_date TIMESTAMP\n",
    ")\n",
    "USING DELTA;\n",
    "\n",
    "-- 2. Perform the Upsert (Merge)\n",
    "-- This logic says: \"If the record exists, update it. If not, insert it.\"\n",
    "MERGE INTO silver_crypto_prices AS target\n",
    "USING (\n",
    "  SELECT \n",
    "    coin_id,\n",
    "    event_time,\n",
    "    date,\n",
    "    price_usd,\n",
    "    current_timestamp() as ingest_date\n",
    "  FROM bronze_crypto_prices\n",
    "  WHERE price_usd IS NOT NULL  -- Simple Data Quality Check\n",
    ") AS source\n",
    "ON target.coin_id = source.coin_id AND target.event_time = source.event_time\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.price_usd = source.price_usd, target.ingest_date = source.ingest_date\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *;\n",
    "\n",
    "-- 3. Verify\n",
    "SELECT * FROM silver_crypto_prices ORDER BY event_time DESC LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8a91486-32a2-4633-a26c-c00da9d3787e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Goal: Ensure high data quality. We will use a MERGE (Upsert) operation. This ensures that if you run the pipeline multiple times, you don't get duplicate rows for the same timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe0ed5a-cc59-4601-b0d4-47ddba6d9db2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Gold Layer (Aggregation-Analytics)"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5628140147600067,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Ingest_Crypto_Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
